{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNovvMmeOGSZ",
        "outputId": "b7af5920-c9ec-4436-84ee-eac7bd9794d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "from scipy import linalg as la\n",
        "from scipy import signal\n",
        "from scipy import special as ss\n",
        "import torch\n",
        "import torch.nn.init as init\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"The Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "atoJ89HjOGSa"
      },
      "outputs": [],
      "source": [
        "class LstmCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, activation = \"tanh\"):\n",
        "        super(LstmCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size, hidden_size)\n",
        "        self.h2h = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = activation\n",
        "        if self.activation not in [\"tanh\", \"relu\", \"sigmoid\"]:\n",
        "            raise ValueError(\"Invalid nonlinearity selected for RNN. Please use tanh, relu, or sigmoid.\")\n",
        "\n",
        "\n",
        "    def forward(self, x, carry):\n",
        "        '''\n",
        "        Inputs: input (torch tensor) of shape [batchsize, input_size]\n",
        "                hidden state (torch tensor) of shape [batchsize, hiddensize]\n",
        "        Output: output (torch tensor) of shape [batchsize, hiddensize]\n",
        "        '''\n",
        "\n",
        "        # carry\n",
        "        h_t_1, c_t_1 = carry\n",
        "        gates_i = self.i2h(x)\n",
        "        #gates_h = self.h2h(h_t_1)\n",
        "        gates_h = self.h2h(torch.cat((x[:, :1], h_t_1), dim=-1))\n",
        "        # shape: x :-: (batchsize, 1)\n",
        "        # shape: h_t_1 :-: (batchsize, hiddensize)\n",
        "        # shape: c_t_1 :-: (batchsize, hiddensize)\n",
        "        # shape: gates_i :-: (batchsize, hiddensize * 4)\n",
        "        # shape: gates_h :-: (batchsize, hiddensize * 4)\n",
        "        # shape: input_gates :-: (batchsize, hiddensize)\n",
        "        # shape: c_t :-: (batchsize, hiddensize)\n",
        "        output_gate = gates_i + gates_h\n",
        "\n",
        "        o_t = torch.sigmoid(output_gate)\n",
        "\n",
        "        c_t = c_t_1\n",
        "\n",
        "        if self.activation == \"tanh\":\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "        if self.activation == \"relu\":\n",
        "            h_t = o_t * torch.relu(c_t)\n",
        "        if self.activation == \"sigmoid\":\n",
        "            h_t = o_t * torch.sigmoid(c_t)\n",
        "\n",
        "        return (h_t, c_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kNgzkYsEOGSb"
      },
      "outputs": [],
      "source": [
        "class HippoLegsCell(nn.Module):\n",
        "    '''\n",
        "    Hippo class utilizing legs polynomial\n",
        "    '''\n",
        "\n",
        "    def __init__(self, N, gbt_alpha = 0.5, maxlength = 1024, reconst = False):\n",
        "        super(HippoLegsCell, self).__init__()\n",
        "        self.N = N\n",
        "        self.gbt_alpha = gbt_alpha\n",
        "        self.maxlength = maxlength\n",
        "        A, self._B = self.get_A_and_B(N = self.N)\n",
        "        GBTA, GBTB = self.get_stacked_GBT(A = A, B = self._B)\n",
        "        self.A = torch.from_numpy(GBTA).to(device)\n",
        "        self.B = torch.from_numpy(GBTB).to(device)\n",
        "        self.reconst = reconst\n",
        "\n",
        "    def compute_A(self, n, k):\n",
        "        '''\n",
        "        Computes the values for the HiPPO A matrix row by column\n",
        "        using the piecewise equation on p. 31 eq. 29:\n",
        "                (2n+1)^{1/2} (2k+ 1)^{1/2} if n > k\n",
        "        A_{nk} = n+1                       if n = k,\n",
        "                 0                         if n < k\n",
        "        , where n represents the row and k the columns.\n",
        "\n",
        "        Input:\n",
        "            n (int):\n",
        "                nth row of a square matrix of size N\n",
        "            k (int):\n",
        "                kth column of a square matrix of size N\n",
        "\n",
        "        Output:\n",
        "            Values (float):\n",
        "            Individual values for the elements in the A matrix.\n",
        "        '''\n",
        "        if n > k:\n",
        "            val = np.sqrt(2 * n + 1, dtype = np.float32) * np.sqrt(2 * k + 1, dtype = np.float32)\n",
        "        if n == k:\n",
        "            val = n + 1\n",
        "        if n < k:\n",
        "            val = 0\n",
        "        return val\n",
        "\n",
        "    def compute_B(self, n):\n",
        "        '''\n",
        "        Computes the values for the HiPPO B matrix row by column\n",
        "        using the piecewise equation on p. 31 eq. 29:\n",
        "        B_{n} = (2n+1)^{1/2}\n",
        "\n",
        "        Input:\n",
        "            n (int):\n",
        "                nth column of a square matrix of size N.\n",
        "\n",
        "        Output:\n",
        "            Values (float):\n",
        "            Individual values for the elements in the B matrix.\n",
        "            The next hidden state (aka coefficients representing the function, f(t))\n",
        "        '''\n",
        "        val = np.sqrt(2 * n + 1, dtype = np.float32)\n",
        "        return val\n",
        "\n",
        "    def get_A_and_B(self, N):\n",
        "        '''\n",
        "        Creates the HiPPO A and B matrix given the size N along a single axis of\n",
        "        a square matrix.\n",
        "\n",
        "        Input:\n",
        "            N (int):\n",
        "            Size N of a square matrix along a single axis.\n",
        "\n",
        "        Output:\n",
        "            A (np.ndarray)\n",
        "                shape: (N,N)\n",
        "                the HiPPO A matrix.\n",
        "            B (np.ndarray)\n",
        "                shape: (N,):\n",
        "                The HiPPO B matrix.\n",
        "        '''\n",
        "        A = np.zeros((self.N, self.N), dtype = np.float32)\n",
        "        B = np.zeros((self.N, 1), dtype = np.float32)\n",
        "        for n in range(A.shape[0]):\n",
        "            B[n][0] = self.compute_B(n = n)\n",
        "            for k in range(A.shape[1]):\n",
        "                A[n, k] = self.compute_A(n = n , k = k)\n",
        "        return A  * -1, B\n",
        "\n",
        "    def generalized_bilinear_transform(self, A, B, t, gbt_alpha):\n",
        "        '''\n",
        "        Performs the generalised bilinaer transform from p. 21 eq.13:\n",
        "        c(t + ∆t) − ∆tαAc(t + ∆t) = (I + ∆t(1 − α)A)c(t) + ∆tBf(t)\n",
        "        c(t + ∆t) = (I − ∆tαA)^{−1} (I + ∆t(1 − α)A)c(t) + ∆t(I − ∆tαA)^{−1}Bf(t).\n",
        "        on the HiPPO matrix A and B, transforming them.\n",
        "        Input:\n",
        "            A (np.ndarray):\n",
        "                shape: (N, N)\n",
        "                the HiPPO A matrix\n",
        "            B (np.ndarray):\n",
        "                shape: (N,)\n",
        "                the HiPPO B matrix\n",
        "            Timestep t = 1/input length at t (int):\n",
        "\n",
        "        Output:\n",
        "            GBTA (np.array):\n",
        "                shape: (N, N)\n",
        "                Transformed HiPPO A matrix.\n",
        "\n",
        "            GBTB (np.array):\n",
        "                shape: (N,)\n",
        "                Transformed HiPPO B matrix.\n",
        "        '''\n",
        "        I = np.eye(A.shape[0], dtype = np.float32)\n",
        "        delta_t = 1 / t\n",
        "        EQ13_p1 = I - (delta_t * gbt_alpha * A)\n",
        "        EQ13_p2 = I + (delta_t * (1 - gbt_alpha) * A)\n",
        "        EQA = np.linalg.solve(EQ13_p1, EQ13_p2)\n",
        "        EQB =  np.linalg.solve(EQ13_p1, (delta_t * B))\n",
        "        return EQA, EQB\n",
        "\n",
        "    def get_stacked_GBT(self, A, B):\n",
        "        GBTA_stacked = np.empty((self.maxlength, self.N, self.N), dtype=np.float32)\n",
        "        GBTB_stacked = np.empty((self.maxlength, self.N, 1), dtype=np.float32)\n",
        "        for t in range(1, self.maxlength + 1):\n",
        "            GBTA, GBTB = self.generalized_bilinear_transform(A = A, B = B, t = t, gbt_alpha = self.gbt_alpha)\n",
        "            GBTA_stacked[t-1] = GBTA\n",
        "            GBTB_stacked[t-1] = GBTB\n",
        "        return GBTA_stacked, GBTB_stacked\n",
        "\n",
        "\n",
        "    def reconstruct(self, c, B):\n",
        "        '''\n",
        "        Input:\n",
        "            c (np.ndarray): 2, 1, 32\n",
        "                shape: (batchsize, 1, N_coeffs)\n",
        "                coefficent matrix\n",
        "            B (np.ndarray):\n",
        "                shape: (N, 1)\n",
        "                the discretized B matrix\n",
        "        Returns:\n",
        "            recon (np.ndarray):\n",
        "                shape: (batchsize, maxlength, 1)\n",
        "                Reconstruction matrix.\n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "            vals = np.linspace(0.0, 1.0, self.maxlength)\n",
        "            # c shape from: [batchsize, 1, N_coeffs]\n",
        "            # move to: [batchsize, N_coeffs, 1]\n",
        "            c = torch.moveaxis(c, 1, 2).float()\n",
        "            eval_mat = (self._B * np.float32(ss.eval_legendre(np.expand_dims(np.arange(self.N, dtype = np.float32), -1), 2 * vals - 1))).T\n",
        "            # shape: B :-: (N, 1)\n",
        "            # shape: eval_mat :-:  (maxlen, N)\n",
        "            recon = (torch.tensor(eval_mat).to(device) @ c.to(device))\n",
        "            # shape: recon :-: (batchsize, maxlength, 1\n",
        "            return recon\n",
        "\n",
        "    def forward(self, input, c_t = None, t = 0):\n",
        "        '''\n",
        "        Input:\n",
        "            A (np.ndarray):\n",
        "                shape: (N, N)\n",
        "                the discretized A matrix\n",
        "            B (np.ndarray):\n",
        "                shape: (N, 1)\n",
        "                the discretized B matrix\n",
        "            c_t (np.ndarray): 2, 1, 32\n",
        "                shape: (batch size, 1, N)\n",
        "                the initial hidden state\n",
        "            input (torch.tensor):\n",
        "                shape: (batch size, 1 ,1)\n",
        "                the input sequence\n",
        "        Output:\n",
        "            c (np.array):\n",
        "                shape: (batch size, 1, N)\n",
        "                coefficent matrix c.\n",
        "        '''\n",
        "        batchsize = input.shape[0]\n",
        "        L = input.shape[1]\n",
        "        if c_t is None:\n",
        "            c_t = torch.zeros((batchsize, 1, self.N)).to(device)\n",
        "\n",
        "        if t == 0:\n",
        "            c_t = c_t.float().unsqueeze(1)\n",
        "\n",
        "        c_t = F.linear(c_t.float(), self.A[t]).float() + self.B[t].squeeze(-1) * input\n",
        "        # shape: F.linear(torch.tensor(c_t).float(), torch.tensor(A[t])) :-: (batchsize, 1, N)\n",
        "        # shape: (np.squeeze(B[t], -1) * f_t.numpy()).shape) :-: (batchsize, 1, N)\n",
        "        # shape: A[t] :-: (N, N)\n",
        "        # shape: torch.squeeze(B[t], -1) :-: (N, )\n",
        "        # shape: f_t :-: (batchsize, 1, 1)\n",
        "        # shape: c_t :-: (batchsize, 1, N)\n",
        "        if self.reconst:\n",
        "            # 3. Compute reconstruction r\n",
        "            r =  self.reconstruct(c = c_t, B = self._B)\n",
        "        else:\n",
        "            r = 0\n",
        "        return c_t, r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8TfUz1JxOGSc"
      },
      "outputs": [],
      "source": [
        "class GatedHippoCell(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, hidden_size, gbt_alpha=0.5, maxlength=1024):\n",
        "        super(GatedHippoCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gbt_alpha = gbt_alpha\n",
        "        self.maxlength = maxlength\n",
        "\n",
        "        self.tau = LstmCell(self.hidden_size + 1, self.hidden_size)\n",
        "        self.fc = nn.Linear(self.hidden_size + 1, 1)\n",
        "        self.hippo_t = HippoLegsCell(\n",
        "            N=self.hidden_size, gbt_alpha=self.gbt_alpha, maxlength=self.maxlength\n",
        "        )\n",
        "\n",
        "    def forward(self, x, carry=(None, None), t=0):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the cell such that it performs one time step in the recurrence\n",
        "\n",
        "        Args:\n",
        "          input:\n",
        "            The input at the current time step, shape (N, sequencelen).\n",
        "            example:\n",
        "              sequencelen for mnist = 28 * 28\n",
        "          carry:\n",
        "            The hidden state and optionally a cell state in a tuple that is carried between recurrent steps, shape h_t (N, hiddensize)\n",
        "\n",
        "        Returns:\n",
        "          The carry of the cell for the given recurrence's time step\n",
        "        \"\"\"\n",
        "\n",
        "        # shape: inputs :-: (N, sequencelen) sequencelen for mnist = 28 * 28\n",
        "        # shape: h_t :-: (N, hiddensize)\n",
        "        # shape : carry[0]  :-: h_t.shape or (N, hiddensize)\n",
        "        if carry[0] is None:\n",
        "            carry = (\n",
        "                torch.zeros(x.shape[0], 1, self.hidden_size).to(device),\n",
        "                torch.zeros(x.shape[0], 1, self.hidden_size).to(device),\n",
        "            )\n",
        "        h_t, c_t_1 = carry\n",
        "\n",
        "        if t == 0:\n",
        "            tau_x = torch.cat((x, c_t_1), dim=-1)\n",
        "        if t > 0:\n",
        "            tau_x = torch.cat((x, c_t_1.squeeze(1)), dim=-1)\n",
        "\n",
        "        h_t, _ = self.tau(x = tau_x, carry=(carry[0], carry[1].squeeze(1)))\n",
        "\n",
        "        # tau is an lstm cell that only retains a single gate i.e., outputgate:\n",
        "        # shape: x :-: (batchsize, 1)\n",
        "        # shape: h_t_1 :-: (batchsize, hiddensize)\n",
        "        # shape: c_t_1 :-: (batchsize, hiddensize)\n",
        "        # shape: gates_i :-: (batchsize, hiddensize * 4)\n",
        "        # shape: gates_h :-: (batchsize, hiddensize * 4)\n",
        "        # shape: input_gates :-: (batchsize, hiddensize)\n",
        "        # shape: c_t :-: (batchsize, hiddensize)\n",
        "\n",
        "        f_t = self.fc(torch.cat((x, h_t), dim=-1))\n",
        "        # shape: h_t :-: (batchsize, hiddensize)\n",
        "        # shape: h_t :-: (batchsize, hiddensize)\n",
        "\n",
        "        c_t, _ = self.hippo_t(f_t.unsqueeze(-1), c_t_1, t=t)\n",
        "\n",
        "        # update previous coefficents to be current coefficents\n",
        "        c_t_1 = c_t\n",
        "        carry = (h_t, c_t_1)\n",
        "\n",
        "\n",
        "        return (h_t, c_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YDMO2NP_OGSd"
      },
      "outputs": [],
      "source": [
        "class HippoRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A PyTorch Module representing a Hippocampal Recurrent Neural Network. This model combines a GatedHippoCell\n",
        "    with a MLP (Multi Layer Perceptron) for output prediction.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_size: The size of the hidden state.\n",
        "        output_size: The size of the output.\n",
        "        cell: A GatedHippoCell instance.\n",
        "        mlp: A Multi Layer Perceptron used in the forward pass for output prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        The __init__ method for the HippoRNN class. Initializes the GatedHippoCell and MLP.\n",
        "\n",
        "        Args:\n",
        "            input_size: The dimensionality of the input data.\n",
        "            hidden_size: The size of the hidden states in the GatedHippoCell.\n",
        "            output_size: The size of the output of MLP.\n",
        "        \"\"\"\n",
        "        super(HippoRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.cell = GatedHippoCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, 10)\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for layer_name, layer in self.named_children():\n",
        "            if layer_name in ['i2h', 'h2h']:\n",
        "                for param_name, param in layer.named_parameters():\n",
        "                    if 'weight' in param_name:\n",
        "                        init.uniform_(param, -std, std)\n",
        "                    elif 'bias' in param_name:\n",
        "                        init.zeros_(param)\n",
        "\n",
        "    def forward(self, xs, carry=(None, None)):\n",
        "        \"\"\"\n",
        "        Performs the forward pass for the entire sequence of data.\n",
        "\n",
        "        Args:\n",
        "            inputs: The input data for the whole sequence, shape (batch_size, sequence_len, input_size).\n",
        "            carry: The hidden state and optionally a cell state in a tuple that is carried between recurrent steps,\n",
        "            shape h_t (batch_size, hidden_size).\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the final prediction of the MLP and a tensor of all cell states in the sequence.\n",
        "        \"\"\"\n",
        "        if carry[0] is None:\n",
        "            carry = (\n",
        "                torch.zeros(xs.shape[0], self.hidden_size).to(device),\n",
        "                torch.zeros(xs.shape[0], self.hidden_size).to(device),\n",
        "            )\n",
        "\n",
        "        for t in range(xs.size(1)):\n",
        "\n",
        "            # shape xs :=: (batchsize, 28*28, 1)\n",
        "            # shape xs[:, t, :] :=: (batchsize, 1)\n",
        "\n",
        "            # Input to hippo cell should be :-: shape: f_t :-: (batchsize, 1, 1)\n",
        "            # Carry to hippo cell should be :-: shape: c_t :-: (batchsize, 1, N)\n",
        "            # Carry to hippo cell should be :-: shape: h_t :-: (batchsize, 1, N)\n",
        "            carry = self.cell(x=xs[:, t, :], carry=carry, t=t)\n",
        "            c_t = carry[1]\n",
        "        return self.fc(c_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RkgZaoeeOGSd",
        "outputId": "92835978-55d6-4cec-a3e1-4f0ee2008277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 104606142.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 63190241.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31926522.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16652560.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "HippoRNN(\n",
            "  (cell): GatedHippoCell(\n",
            "    (tau): LstmCell(\n",
            "      (i2h): Linear(in_features=513, out_features=512, bias=True)\n",
            "      (h2h): Linear(in_features=513, out_features=512, bias=True)\n",
            "    )\n",
            "    (fc): Linear(in_features=513, out_features=1, bias=True)\n",
            "    (hippo_t): HippoLegsCell()\n",
            "  )\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "Epoch:1   Train[Loss:1.0989  Accuracy:0.7167]\n",
            "Epoch:2   Train[Loss:0.2695  Accuracy:0.9206]\n",
            "Epoch:3   Train[Loss:0.1745  Accuracy:0.9486]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-98b739cc3358>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtrain_loss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch:{epoch + 1}   Train[Loss:{train_loss_value}  Accuracy:{train_accuracy}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-98b739cc3358>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, optimizer, loss_f)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# get class probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train (data_loader, model, optimizer, loss_f):\n",
        "    \"\"\"\n",
        "    Input: train loader (torch loader), model (torch model), optimizer (torch optimizer)\n",
        "          loss function (torch custom yolov1 loss).\n",
        "    Output: loss (torch float).\n",
        "    \"\"\"\n",
        "    loss_lst = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.train()\n",
        "    for batch_idx, (x, y) in enumerate(data_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x = x.view(batch_size, -1, 1)\n",
        "        out = model(x)\n",
        "        del x\n",
        "        loss_val = loss_f(out.squeeze(1), y)\n",
        "        loss_lst.append(float(loss_val.item()))\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "        # get class probabilities\n",
        "        classprobs = F.softmax(out.squeeze(1), dim=1)\n",
        "        # get class predictions\n",
        "        preds = classprobs.argmax(dim=1)\n",
        "        # compute accuracy\n",
        "        total += y.size(0)\n",
        "        correct += (preds == y).sum().item()\n",
        "\n",
        "    # Compute average loss and accuracy\n",
        "    loss_val = round(sum(loss_lst) / len(loss_lst), 4)\n",
        "    accuracy = round(correct / total, 4)\n",
        "    return loss_val, accuracy\n",
        "\n",
        "batch_size = 64\n",
        "weight_decay = 0\n",
        "\n",
        "epochs = 50\n",
        "nworkers = 2\n",
        "lr = 1e-3\n",
        "pin_memory = True\n",
        "data_dir =  'data/'\n",
        "\n",
        "input_size = 1\n",
        "hidden_size = 512\n",
        "output_size = 10\n",
        "train_dataset = datasets.MNIST(root=data_dir,\n",
        "                               train=True,\n",
        "                               transform=T.Compose([T.ToTensor(),\n",
        "                                                   T.Normalize((0.5,), (0.5,))]),\n",
        "                               download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset = train_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = True, num_workers = 2, drop_last = True)\n",
        "\n",
        "model = HippoRNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
        "print(model)\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "for epoch in range(epochs):\n",
        "    train_loss_value, train_accuracy = train(train_loader, model, optimizer, loss_f)\n",
        "    print(f\"Epoch:{epoch + 1}   Train[Loss:{train_loss_value}  Accuracy:{train_accuracy}]\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}